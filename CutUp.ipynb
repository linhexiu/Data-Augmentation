{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJlbt2c9I0HCvhSbspOTE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f99241d1532f4c7c83dc0435e083f7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_309a4fd95ad6466baf6df4d5e296942d",
              "IPY_MODEL_8dea24aa1e314c9d804d6ce8861d5827",
              "IPY_MODEL_c3c6236e84dd4585bf911299875a9a4e"
            ],
            "layout": "IPY_MODEL_79487081224f4f64ac20a557eb8e0dc3"
          }
        },
        "309a4fd95ad6466baf6df4d5e296942d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2808190afc0746e483fb56fbccae0384",
            "placeholder": "​",
            "style": "IPY_MODEL_cb0d075856254f679d42282ccb8930af",
            "value": "100%"
          }
        },
        "8dea24aa1e314c9d804d6ce8861d5827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c203e138a641426589b88cce7d67168e",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_beab08589c4e4185acfb46e15a3353dd",
            "value": 170498071
          }
        },
        "c3c6236e84dd4585bf911299875a9a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab5ded0aea854c40bce5253f5b3fb186",
            "placeholder": "​",
            "style": "IPY_MODEL_1810f34b69594fc6814671a7b5ac5b49",
            "value": " 170498071/170498071 [00:14&lt;00:00, 12894228.01it/s]"
          }
        },
        "79487081224f4f64ac20a557eb8e0dc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2808190afc0746e483fb56fbccae0384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb0d075856254f679d42282ccb8930af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c203e138a641426589b88cce7d67168e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beab08589c4e4185acfb46e15a3353dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab5ded0aea854c40bce5253f5b3fb186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1810f34b69594fc6814671a7b5ac5b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhexiu/Data-Augmentation/blob/main/CutUp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CutUp:数据增强策略； 以下主要用于自我学习整理； 代码来源：https://github.com/hysts/pytorch_cutmix\n",
        "```\n",
        "acc. \n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pRR8wPgYJW4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi -i 0\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE6c1BsJJX1_",
        "outputId": "666bd31a-e469-4c7f-aa79-7a609fda1d07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 17 05:59:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import argparse\n",
        "import importlib\n",
        "import json\n",
        "import logging\n",
        "import pathlib\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "try:\n",
        "    from tensorboardX import SummaryWriter\n",
        "    is_tensorboard_available = True\n",
        "except Exception:\n",
        "    is_tensorboard_available = False"
      ],
      "metadata": {
        "id": "8Lw9sl9ZKGcX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='[%(asctime)s %(name)s %(levelname)s] - %(message)s',\n",
        "    datefmt='%Y/%m/%d %H:%M:%S',\n",
        "    level=logging.DEBUG)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "def str2bool(s):\n",
        "    if s.lower() == 'true':\n",
        "        return True\n",
        "    elif s.lower() == 'false':\n",
        "        return False\n",
        "    else:\n",
        "        raise RuntimeError('Boolean value expected')"
      ],
      "metadata": {
        "id": "n4Cvn7qKLUtf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # model config\n",
        "    parser.add_argument(\n",
        "        '--block_type',\n",
        "        type=str,\n",
        "        default='basic',\n",
        "        choices=['basic', 'bottleneck'])\n",
        "    parser.add_argument('--depth', type=int,default=20)\n",
        "    parser.add_argument('--base_channels', type=int, default=64)\n",
        "\n",
        "    # cutmix\n",
        "    parser.add_argument('--use_cutmix', action='store_false')\n",
        "    parser.add_argument('--cutmix_alpha', type=float, default=1.0)\n",
        "\n",
        "    # run config\n",
        "    parser.add_argument('--outdir', type=str,default='results/cutmix')\n",
        "    parser.add_argument('--seed', type=int, default=7)\n",
        "    parser.add_argument('--num_workers', type=int, default=4)\n",
        "    parser.add_argument('--device', type=str, default='cuda')\n",
        "\n",
        "    # optim config\n",
        "    parser.add_argument('--epochs', type=int, default=300)\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--base_lr', type=float, default=0.2)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-4)\n",
        "    parser.add_argument('--momentum', type=float, default=0.9)\n",
        "    parser.add_argument('--nesterov', type=str2bool, default=True)\n",
        "    parser.add_argument(\n",
        "        '--scheduler',\n",
        "        type=str,\n",
        "        default='cosine',\n",
        "        choices=['multistep', 'cosine'])\n",
        "    parser.add_argument('--milestones', type=str, default='[150, 225]')\n",
        "    parser.add_argument('--lr_decay', type=float, default=0.1)\n",
        "\n",
        "    # TensorBoard\n",
        "    parser.add_argument(\n",
        "        '--no-tensorboard', dest='tensorboard', action='store_false')\n",
        "\n",
        "    args = parser.parse_args('')\n",
        "    if not is_tensorboard_available:\n",
        "        args.tensorboard = False\n",
        "\n",
        "    model_config = OrderedDict([\n",
        "        ('arch', 'resnet_preact'),\n",
        "        ('block_type', args.block_type),\n",
        "        ('depth', args.depth),\n",
        "        ('base_channels', args.base_channels),\n",
        "        ('input_shape', (1, 3, 32, 32)),\n",
        "        ('n_classes', 10),\n",
        "    ])\n",
        "\n",
        "    optim_config = OrderedDict([\n",
        "        ('epochs', args.epochs),\n",
        "        ('batch_size', args.batch_size),\n",
        "        ('base_lr', args.base_lr),\n",
        "        ('weight_decay', args.weight_decay),\n",
        "        ('momentum', args.momentum),\n",
        "        ('nesterov', args.nesterov),\n",
        "        ('scheduler', args.scheduler),\n",
        "        ('milestones', json.loads(args.milestones)),\n",
        "        ('lr_decay', args.lr_decay),\n",
        "    ])\n",
        "\n",
        "    data_config = OrderedDict([\n",
        "        ('dataset', 'CIFAR10'),\n",
        "        ('use_cutmix', args.use_cutmix),\n",
        "        ('cutmix_alpha', args.cutmix_alpha),\n",
        "    ])\n",
        "\n",
        "    run_config = OrderedDict([\n",
        "        ('seed', args.seed),\n",
        "        ('outdir', args.outdir),\n",
        "        ('num_workers', args.num_workers),\n",
        "        ('device', args.device),\n",
        "        ('tensorboard', args.tensorboard),\n",
        "    ])\n",
        "\n",
        "    config = OrderedDict([\n",
        "        ('model_config', model_config),\n",
        "        ('optim_config', optim_config),\n",
        "        ('data_config', data_config),\n",
        "        ('run_config', run_config),\n",
        "    ])\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "94QNa-tHLdJG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, num):\n",
        "        self.val = val\n",
        "        self.sum += val * num\n",
        "        self.count += num\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "yej4ChXkT4b4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, model, optimizer, criterion, train_loader, run_config,\n",
        "          writer):\n",
        "    global global_step\n",
        "\n",
        "    logger.info('Train {}'.format(epoch))\n",
        "\n",
        "    model.train()\n",
        "    device = torch.device(run_config['device'])\n",
        "\n",
        "    loss_meter = AverageMeter()\n",
        "    accuracy_meter = AverageMeter()\n",
        "    start = time.time()\n",
        "    for step, (data, targets) in enumerate(train_loader):\n",
        "        global_step += 1\n",
        "\n",
        "        if run_config['tensorboard'] and step == 0:\n",
        "            image = torchvision.utils.make_grid(\n",
        "                data, normalize=True, scale_each=True)\n",
        "            writer.add_image('Train/Image', image, epoch)\n",
        "\n",
        "        data = data.to(device)\n",
        "        if isinstance(targets, (tuple, list)):\n",
        "            targets1, targets2, lam = targets\n",
        "            targets = (targets1.to(device), targets2.to(device), lam)\n",
        "        else:\n",
        "            targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        loss_ = loss.item()\n",
        "\n",
        "        num = data.size(0)\n",
        "        if isinstance(targets, (tuple, list)):\n",
        "            targets1, targets2, lam = targets\n",
        "            correct1 = preds.eq(targets1).sum().item()\n",
        "            correct2 = preds.eq(targets2).sum().item()\n",
        "            accuracy = (lam * correct1 + (1 - lam) * correct2) / num\n",
        "        else:\n",
        "            correct_ = preds.eq(targets).sum().item()\n",
        "            accuracy = correct_ / num\n",
        "\n",
        "        loss_meter.update(loss_, num)\n",
        "        accuracy_meter.update(accuracy, num)\n",
        "\n",
        "        if run_config['tensorboard']:\n",
        "            writer.add_scalar('Train/RunningLoss', loss_, global_step)\n",
        "            writer.add_scalar('Train/RunningAccuracy', accuracy, global_step)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            logger.info('Epoch {} Step {}/{} '\n",
        "                        'Loss {:.4f} ({:.4f}) '\n",
        "                        'Accuracy {:.4f} ({:.4f})'.format(\n",
        "                            epoch,\n",
        "                            step,\n",
        "                            len(train_loader),\n",
        "                            loss_meter.val,\n",
        "                            loss_meter.avg,\n",
        "                            accuracy_meter.val,\n",
        "                            accuracy_meter.avg,\n",
        "                        ))\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    logger.info('Elapsed {:.2f}'.format(elapsed))\n",
        "\n",
        "    if run_config['tensorboard']:\n",
        "        writer.add_scalar('Train/Loss', loss_meter.avg, epoch)\n",
        "        writer.add_scalar('Train/Accuracy', accuracy_meter.avg, epoch)\n",
        "        writer.add_scalar('Train/Time', elapsed, epoch)\n",
        "\n",
        "    train_log = OrderedDict({\n",
        "        'epoch':\n",
        "        epoch,\n",
        "        'train':\n",
        "        OrderedDict({\n",
        "            'loss': loss_meter.avg,\n",
        "            'accuracy': accuracy_meter.avg,\n",
        "            'time': elapsed,\n",
        "        }),\n",
        "    })\n",
        "    return train_log"
      ],
      "metadata": {
        "id": "pkiH-1dbOHGt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch, model, criterion, test_loader, run_config, writer):\n",
        "    logger.info('Test {}'.format(epoch))\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(run_config['device'])\n",
        "\n",
        "    loss_meter = AverageMeter()\n",
        "    correct_meter = AverageMeter()\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for step, (data, targets) in enumerate(test_loader):\n",
        "            if run_config['tensorboard'] and epoch == 0 and step == 0:\n",
        "                image = torchvision.utils.make_grid(\n",
        "                    data, normalize=True, scale_each=True)\n",
        "                writer.add_image('Test/Image', image, epoch)\n",
        "\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            loss_ = loss.item()\n",
        "            correct_ = preds.eq(targets).sum().item()\n",
        "            num = data.size(0)\n",
        "\n",
        "            loss_meter.update(loss_, num)\n",
        "            correct_meter.update(correct_, 1)\n",
        "\n",
        "    accuracy = correct_meter.sum / len(test_loader.dataset)\n",
        "\n",
        "    logger.info('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "        epoch, loss_meter.avg, accuracy))\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    logger.info('Elapsed {:.2f}'.format(elapsed))\n",
        "\n",
        "    if run_config['tensorboard']:\n",
        "        if epoch > 0:\n",
        "            writer.add_scalar('Test/Loss', loss_meter.avg, epoch)\n",
        "        writer.add_scalar('Test/Accuracy', accuracy, epoch)\n",
        "        writer.add_scalar('Test/Time', elapsed, epoch)\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            writer.add_histogram(name, param, global_step)\n",
        "\n",
        "    test_log = OrderedDict({\n",
        "        'epoch':\n",
        "        epoch,\n",
        "        'test':\n",
        "        OrderedDict({\n",
        "            'loss': loss_meter.avg,\n",
        "            'accuracy': accuracy,\n",
        "            'time': elapsed,\n",
        "        }),\n",
        "    })\n",
        "    return test_log"
      ],
      "metadata": {
        "id": "48TR4VDeVLCJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = parse_args()\n",
        "print(config)"
      ],
      "metadata": {
        "id": "-VTuEFbYVpo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a283e02-4169-43cb-d2ae-6e03783c03fa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('model_config', OrderedDict([('arch', 'resnet_preact'), ('block_type', 'basic'), ('depth', 20), ('base_channels', 64), ('input_shape', (1, 3, 32, 32)), ('n_classes', 10)])), ('optim_config', OrderedDict([('epochs', 300), ('batch_size', 128), ('base_lr', 0.2), ('weight_decay', 0.0001), ('momentum', 0.9), ('nesterov', True), ('scheduler', 'cosine'), ('milestones', [150, 225]), ('lr_decay', 0.1)])), ('data_config', OrderedDict([('dataset', 'CIFAR10'), ('use_cutmix', True), ('cutmix_alpha', 1.0)])), ('run_config', OrderedDict([('seed', 7), ('outdir', 'results/cutmix'), ('num_workers', 4), ('device', 'cuda'), ('tensorboard', False)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(json.dumps(config, indent=2))\n",
        "run_config = config['run_config']\n",
        "optim_config = config['optim_config']\n",
        "data_config = config['data_config']"
      ],
      "metadata": {
        "id": "54jfHMw8XTHJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "seed = run_config['seed']\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "print(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSRzsbQCRBz4",
        "outputId": "c122e84e-4453-4d12-a7c5-cccfa927bbcf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create output directory\n",
        "outdir = pathlib.Path(run_config['outdir'])\n",
        "outdir.mkdir(exist_ok=True, parents=True)\n",
        "print(outdir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1VkP66YRUeg",
        "outputId": "d53e9eb8-afc9-4d0a-dd0a-d327fb75eaf4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results/cutmix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorBoard SummaryWriter\n",
        "writer = SummaryWriter(\n",
        "outdir.as_posix()) if run_config['tensorboard'] else None"
      ],
      "metadata": {
        "id": "roB0RUcjRdkk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save config as json file in output directory\n",
        "outpath = outdir / 'config.json'\n",
        "with open(outpath, 'w') as fout:\n",
        "  json.dump(config, fout, indent=2)\n",
        "print(outpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wp65_2uRl42",
        "outputId": "f5049de0-2c43-4e0e-840f-183dbfbe79a1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results/cutmix/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cutmix(batch, alpha):\n",
        "    data, targets = batch\n",
        "\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets = targets[indices]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "    image_h, image_w = data.shape[2:] # 32，32\n",
        "    cx = np.random.uniform(0, image_w) \n",
        "    cy = np.random.uniform(0, image_h)\n",
        "    w = image_w * np.sqrt(1 - lam) # \n",
        "    h = image_h * np.sqrt(1 - lam) # \n",
        "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
        "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
        "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
        "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
        "\n",
        "    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1] # 重点\n",
        "    targets = (targets, shuffled_targets, lam)\n",
        "\n",
        "    return data, targets"
      ],
      "metadata": {
        "id": "7sFJIIbmSz0j"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CutMixCollator:\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
        "        batch = cutmix(batch, self.alpha)\n",
        "        return batch"
      ],
      "metadata": {
        "id": "fbz5AEIdSwWp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get data\n",
        "def get_loader(batch_size, num_workers, config):\n",
        "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "    std = np.array([0.2470, 0.2435, 0.2616])\n",
        "\n",
        "    train_transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.RandomCrop(32, padding=4),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean, std),\n",
        "    ])\n",
        "    test_transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    if config['use_cutmix']:\n",
        "        collator = CutMixCollator(config['cutmix_alpha'])\n",
        "    else:\n",
        "        collator = torch.utils.data.dataloader.default_collate\n",
        "\n",
        "    dataset_dir = '~/.torchvision/datasets/CIFAR10'\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        dataset_dir, train=True, transform=train_transform, download=True)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        dataset_dir, train=False, transform=test_transform, download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collator, # 整理数据\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "KcM2IXSIRyQN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaders\n",
        "train_loader, test_loader = get_loader(\n",
        "optim_config['batch_size'], run_config['num_workers'], data_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "f99241d1532f4c7c83dc0435e083f7a1",
            "309a4fd95ad6466baf6df4d5e296942d",
            "8dea24aa1e314c9d804d6ce8861d5827",
            "c3c6236e84dd4585bf911299875a9a4e",
            "79487081224f4f64ac20a557eb8e0dc3",
            "2808190afc0746e483fb56fbccae0384",
            "cb0d075856254f679d42282ccb8930af",
            "c203e138a641426589b88cce7d67168e",
            "beab08589c4e4185acfb46e15a3353dd",
            "ab5ded0aea854c40bce5253f5b3fb186",
            "1810f34b69594fc6814671a7b5ac5b49"
          ]
        },
        "id": "B3g3UD5_Rvkq",
        "outputId": "ee292a09-e273-4aa1-f55c-63166fce1f26"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/.torchvision/datasets/CIFAR10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f99241d1532f4c7c83dc0435e083f7a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.torchvision/datasets/CIFAR10/cifar-10-python.tar.gz to /root/.torchvision/datasets/CIFAR10\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def initialize_weights(module):\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(module.weight.data, mode='fan_out')\n",
        "    elif isinstance(module, nn.BatchNorm2d):\n",
        "        module.weight.data.fill_(1)\n",
        "        module.bias.data.zero_()\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        module.bias.data.zero_()\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride,\n",
        "                 remove_first_relu,\n",
        "                 add_last_bn,\n",
        "                 preact=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self._remove_first_relu = remove_first_relu\n",
        "        self._add_last_bn = add_last_bn\n",
        "        self._preact = preact\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,  # downsample with first conv\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "\n",
        "        if add_last_bn:\n",
        "            self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module(\n",
        "                'conv',\n",
        "                nn.Conv2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,  # downsample\n",
        "                    padding=0,\n",
        "                    bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self._preact:\n",
        "            x = F.relu(\n",
        "                self.bn1(x), inplace=True)  # shortcut after preactivation\n",
        "            y = self.conv1(x)\n",
        "        else:\n",
        "            # preactivation only for residual path\n",
        "            y = self.bn1(x)\n",
        "            if not self._remove_first_relu:\n",
        "                y = F.relu(y, inplace=True)\n",
        "            y = self.conv1(y)\n",
        "\n",
        "        y = F.relu(self.bn2(y), inplace=True)\n",
        "        y = self.conv2(y)\n",
        "\n",
        "        if self._add_last_bn:\n",
        "            y = self.bn3(y)\n",
        "\n",
        "        y += self.shortcut(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride,\n",
        "                 remove_first_relu,\n",
        "                 add_last_bn,\n",
        "                 preact=False):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "\n",
        "        self._remove_first_relu = remove_first_relu\n",
        "        self._add_last_bn = add_last_bn\n",
        "        self._preact = preact\n",
        "\n",
        "        bottleneck_channels = out_channels // self.expansion\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            bottleneck_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            bottleneck_channels,\n",
        "            bottleneck_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,  # downsample with 3x3 conv\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(bottleneck_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            bottleneck_channels,\n",
        "            out_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False)\n",
        "\n",
        "        if add_last_bn:\n",
        "            self.bn4 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()  # identity\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module(\n",
        "                'conv',\n",
        "                nn.Conv2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,  # downsample\n",
        "                    padding=0,\n",
        "                    bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self._preact:\n",
        "            x = F.relu(\n",
        "                self.bn1(x), inplace=True)  # shortcut after preactivation\n",
        "            y = self.conv1(x)\n",
        "        else:\n",
        "            # preactivation only for residual path\n",
        "            y = self.bn1(x)\n",
        "            if not self._remove_first_relu:\n",
        "                y = F.relu(y, inplace=True)\n",
        "            y = self.conv1(y)\n",
        "\n",
        "        y = F.relu(self.bn2(y), inplace=True)\n",
        "        y = self.conv2(y)\n",
        "        y = F.relu(self.bn3(y), inplace=True)\n",
        "        y = self.conv3(y)\n",
        "\n",
        "        if self._add_last_bn:\n",
        "            y = self.bn4(y)\n",
        "\n",
        "        y += self.shortcut(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        input_shape = config['input_shape']\n",
        "        n_classes = config['n_classes']\n",
        "\n",
        "        base_channels = config['base_channels']\n",
        "        self._remove_first_relu = False\n",
        "        self._add_last_bn = False\n",
        "        block_type = config['block_type']\n",
        "        depth = config['depth']\n",
        "        preact_stage = [True, True, True]\n",
        "\n",
        "        assert block_type in ['basic', 'bottleneck']\n",
        "        if block_type == 'basic':\n",
        "            block = BasicBlock\n",
        "            n_blocks_per_stage = (depth - 2) // 6\n",
        "            assert n_blocks_per_stage * 6 + 2 == depth\n",
        "        else:\n",
        "            block = BottleneckBlock\n",
        "            n_blocks_per_stage = (depth - 2) // 9\n",
        "            assert n_blocks_per_stage * 9 + 2 == depth\n",
        "\n",
        "        n_channels = [\n",
        "            base_channels,\n",
        "            base_channels * 2 * block.expansion,\n",
        "            base_channels * 4 * block.expansion,\n",
        "        ]\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            input_shape[1],\n",
        "            n_channels[0],\n",
        "            kernel_size=(3, 3),\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "\n",
        "        self.stage1 = self._make_stage(\n",
        "            n_channels[0],\n",
        "            n_channels[0],\n",
        "            n_blocks_per_stage,\n",
        "            block,\n",
        "            stride=1,\n",
        "            preact=preact_stage[0])\n",
        "        self.stage2 = self._make_stage(\n",
        "            n_channels[0],\n",
        "            n_channels[1],\n",
        "            n_blocks_per_stage,\n",
        "            block,\n",
        "            stride=2,\n",
        "            preact=preact_stage[1])\n",
        "        self.stage3 = self._make_stage(\n",
        "            n_channels[1],\n",
        "            n_channels[2],\n",
        "            n_blocks_per_stage,\n",
        "            block,\n",
        "            stride=2,\n",
        "            preact=preact_stage[2])\n",
        "        self.bn = nn.BatchNorm2d(n_channels[2])\n",
        "\n",
        "        # compute conv feature size\n",
        "        with torch.no_grad():\n",
        "            self.feature_size = self._forward_conv(\n",
        "                torch.zeros(*input_shape)).view(-1).shape[0]\n",
        "\n",
        "        self.fc = nn.Linear(self.feature_size, n_classes)\n",
        "\n",
        "        # initialize weights\n",
        "        self.apply(initialize_weights)\n",
        "\n",
        "    def _make_stage(self, in_channels, out_channels, n_blocks, block, stride,\n",
        "                    preact):\n",
        "        stage = nn.Sequential()\n",
        "        for index in range(n_blocks):\n",
        "            block_name = 'block{}'.format(index + 1)\n",
        "            if index == 0:\n",
        "                stage.add_module(\n",
        "                    block_name,\n",
        "                    block(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        stride=stride,\n",
        "                        remove_first_relu=self._remove_first_relu,\n",
        "                        add_last_bn=self._add_last_bn,\n",
        "                        preact=preact))\n",
        "            else:\n",
        "                stage.add_module(\n",
        "                    block_name,\n",
        "                    block(\n",
        "                        out_channels,\n",
        "                        out_channels,\n",
        "                        stride=1,\n",
        "                        remove_first_relu=self._remove_first_relu,\n",
        "                        add_last_bn=self._add_last_bn,\n",
        "                        preact=False))\n",
        "        return stage\n",
        "\n",
        "    def _forward_conv(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = F.relu(\n",
        "            self.bn(x),\n",
        "            inplace=True)  # apply BN and ReLU before average pooling\n",
        "        x = F.adaptive_avg_pool2d(x, output_size=1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._forward_conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "iFJJq6wIWaHQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(config):\n",
        "  net=Network(config)\n",
        "  return net"
      ],
      "metadata": {
        "id": "XALDVAh5WrGC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = load_model(config['model_config'])\n",
        "model.to(torch.device(run_config['device']))\n",
        "n_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
        "logger.info('n_params: {}'.format(n_params))\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umcW50BjU2XW",
        "outputId": "deeee65f-5baf-4318-a662-963c3a378ad4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (stage1): Sequential(\n",
            "    (block1): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block2): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block3): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (stage2): Sequential(\n",
            "    (block1): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (block2): BasicBlock(\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block3): BasicBlock(\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (stage3): Sequential(\n",
            "    (block1): BasicBlock(\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (block2): BasicBlock(\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block3): BasicBlock(\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "Network(\n",
            "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (stage1): Sequential(\n",
            "    (block1): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block2): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block3): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (stage2): Sequential(\n",
            "    (block1): BasicBlock(\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (block2): BasicBlock(\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block3): BasicBlock(\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (stage3): Sequential(\n",
            "    (block1): BasicBlock(\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (block2): BasicBlock(\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (block3): BasicBlock(\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CutMixCriterion:\n",
        "    def __init__(self, reduction):\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
        "\n",
        "    def __call__(self, preds, targets):\n",
        "        targets1, targets2, lam = targets\n",
        "        return lam * self.criterion(\n",
        "            preds, targets1) + (1 - lam) * self.criterion(preds, targets2)"
      ],
      "metadata": {
        "id": "-58w_y7TXXg5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if data_config['use_cutmix']:\n",
        "  train_criterion = CutMixCriterion(reduction='mean')\n",
        "else:\n",
        "  train_criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "test_criterion = nn.CrossEntropyLoss(reduction='mean')"
      ],
      "metadata": {
        "id": "3Raa70pKXKfb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=optim_config['base_lr'],\n",
        "    momentum=optim_config['momentum'],\n",
        "    weight_decay=optim_config['weight_decay'],\n",
        "    nesterov=optim_config['nesterov'])\n",
        "if optim_config['scheduler'] == 'multistep':\n",
        "  scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "      optimizer,\n",
        "      milestones=optim_config['milestones'],\n",
        "      gamma=optim_config['lr_decay'])\n",
        "else:\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "      optimizer, optim_config['epochs'], 0)"
      ],
      "metadata": {
        "id": "_9THVtcAXeR-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run test before start training\n",
        "test(0, model, test_criterion, test_loader, run_config, writer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJOXgKJzYND6",
        "outputId": "d4f91582-a03c-4372-c87e-7c9779ab0bcd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('epoch', 0),\n",
              "             ('test',\n",
              "              OrderedDict([('loss', 4.584025408935547),\n",
              "                           ('accuracy', 0.1),\n",
              "                           ('time', 9.141523122787476)]))])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.config.application import indent\n",
        "epoch_logs = []\n",
        "for epoch in range(1, optim_config['epochs'] + 1):\n",
        "  scheduler.step()\n",
        "  train_log=train(epoch, model, optimizer, train_criterion, train_loader, run_config, writer)\n",
        "  test_log = test(epoch, model, test_criterion, test_loader, run_config,\n",
        "                        writer)\n",
        "  epoch_log = train_log.copy()\n",
        "  epoch_log.update(test_log)\n",
        "  epoch_logs.append(epoch_log)\n",
        "  with open(outdir / 'log.json', 'w') as fout:\n",
        "    json.dump(epoch_logs, fout, indent=2)\n",
        "  state = OrderedDict([\n",
        "            ('config', config),\n",
        "            ('state_dict', model.state_dict()),\n",
        "            ('optimizer', optimizer.state_dict()),\n",
        "            ('epoch', epoch),\n",
        "            ('accuracy', test_log['test']['accuracy']),\n",
        "        ])\n",
        "  model_path = outdir / 'model_state.pth'\n",
        "  torch.save(state, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LGEz76vYsIj",
        "outputId": "3b0d9561-43d9-4508-c9cf-07404a667dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    }
  ]
}